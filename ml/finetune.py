# -*- coding: utf-8 -*-
"""Finetune

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q1aSyzBIYD8z_kmtGlYMCMfY7BjpT_3A
"""

import torch
import time
import numpy as np
import os
import random
import matplotlib.pyplot as plt
from sonoModel import SonoModel
from sonoModelLive import SonoModelLive
from sonoDataset import SonoDataset
from torch.utils.data import Dataset, DataLoader, sampler, TensorDataset, random_split
from torchvision import datasets, transforms

import torchaudio
import scipy.io.wavfile as wav
from scipy.io.wavfile import read
import itertools
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import seaborn as sns
from sklearn.metrics import confusion_matrix

DEVICE = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")
print(DEVICE)


#model = SonoModel().to(DEVICE)
model = SonoModelLive().to(DEVICE)
model.load_state_dict(torch.load("hypatia_model2.pt", weights_only=False), strict=False)

dataseth = SonoDataset(root_dir = '/content/drive/MyDrive/EECS498/Data/abhi_04122025T0124_44100hz')
train_data, test_data = random_split(dataseth, [0.70, 0.30])

cls_cnt = np.zeros(5)
cal_data  = []
while(cls_cnt.sum() < 150):
  random_idx = random.randint(0, len(train_data)-1)
  rnd_image = train_data[random_idx][0]
  rnd_label = train_data[random_idx][1]
  if(cls_cnt[rnd_label] < 30):
    cal_data.append((rnd_image, rnd_label))
    cls_cnt[rnd_label] += 1

X_cal = torch.stack([x for x, _ in cal_data])
print(X_cal.size())
X_cal = X_cal.float()
y_cal = torch.tensor([y for _, y in cal_data]) # Shape: (25,)

calibration_dataset = TensorDataset(X_cal, y_cal)
calibration_dl = DataLoader(calibration_dataset, batch_size=8, shuffle=True, drop_last=True)

# Freeze last layers
for param in model.parameters():
    param.requires_grad = False
for param in model.classifier[16].parameters():
    param.requires_grad = True
for param in model.classifier[17].parameters():
    param.requires_grad = True

for layer in model.classifier:
    if isinstance(layer, nn.Dropout):
        layer.p = 0.0


# Fine tune
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)

model.train()
for epoch in range(25):
    epoch_loss = []
    num_correct = 0
    num_total = 0

    for inputs, labels in calibration_dl:
      inputs = inputs.float().to(DEVICE)
      labels = labels.to(DEVICE)

      optimizer.zero_grad()
      outputs = model(inputs)
      loss = criterion(outputs, labels)
      epoch_loss.append(loss.item())

      loss.backward()
      optimizer.step()

      _, predictions = torch.max(outputs, 1)
      num_correct += (predictions == labels).sum().item()
      num_total += labels.size(0)

    print("Epoch:", epoch)
    print("\tAvg. training loss:", sum(epoch_loss) / len(epoch_loss))
    print("\tTraining accuracy: ", 100 * num_correct/num_total)

# Save the updated model
torch.save(model.state_dict(), "model_finetuned.pt")

